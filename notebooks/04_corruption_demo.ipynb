{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from tqdm import tqdm\n",
    "import torch, numpy as np\n",
    "import argparse\n",
    "from baukit import TraceDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include prompt creation helper functions\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.extract_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_replacement_per_class_intervention(prompt_data, avg_activations, dummy_labels, model, model_config, tokenizer, last_token_only=True):\n",
    "    \"\"\"\n",
    "    Experiment to determine top intervention locations through avg activation replacement. \n",
    "    Performs a systematic sweep over attention heads (layer, head) to track their causal influence on probs of key tokens.\n",
    "\n",
    "    Parameters: \n",
    "    prompt_data: dict containing ICL prompt examples, and template information\n",
    "    avg_activations: avg activation of each attention head in the model taken across n_trials ICL prompts\n",
    "    dummy_labels: labels and indices for a baseline prompt with the same number of example pairs\n",
    "    model: huggingface model\n",
    "    model_config: contains model config information (n layers, n heads, etc.)\n",
    "    tokenizer: huggingface tokenizer\n",
    "    last_token_only: If True, only computes indirect effect for heads at the final token position. If False, computes indirect_effect for heads for all token classes\n",
    "\n",
    "    Returns:   \n",
    "    indirect_effect_storage: torch tensor containing the indirect_effect of each head for each token class.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    print(\"---------------------------Activation Replacement--------------\")\n",
    "    print(f\"prompt_data: {prompt_data}\")\n",
    "    # Get sentence and token labels\n",
    "    query_target_pair = prompt_data['query_target']\n",
    "\n",
    "    query = query_target_pair['input']\n",
    "    token_labels, prompt_string = get_token_meta_labels(prompt_data, tokenizer, query=query)\n",
    "    \n",
    "    print(f\"token_labels: {token_labels}\")\n",
    "    print(f\"prompt_string: {prompt_string}\")\n",
    "    \n",
    "    \n",
    "    idx_map, idx_avg = compute_duplicated_labels(token_labels, dummy_labels)\n",
    "    idx_map = update_idx_map(idx_map, idx_avg)\n",
    "      \n",
    "    sentences = [prompt_string]# * model.config.n_head # batch things by head\n",
    "\n",
    "    # Figure out tokens of interest\n",
    "    tokens_of_interest = [query_target_pair['output']]\n",
    "    if 'llama' in model_config['name_or_path']:\n",
    "        ts = tokenizer(tokens_of_interest, return_tensors='pt').input_ids.squeeze()\n",
    "        if tokenizer.decode(ts[1])=='' or ts[1]==29871: # avoid spacing issues\n",
    "            token_id_of_interest = ts[2]\n",
    "        else:\n",
    "            token_id_of_interest = ts[1]\n",
    "    else:\n",
    "        token_id_of_interest = tokenizer(tokens_of_interest).input_ids[0][:1]\n",
    "        \n",
    "    inputs = tokenizer(sentences, return_tensors='pt').to(device)\n",
    "\n",
    "    # Speed up computation by only computing causal effect at last token\n",
    "    if last_token_only:\n",
    "        token_classes = ['query_predictive']\n",
    "        token_classes_regex = ['query_predictive_token']\n",
    "    # Compute causal effect for all token classes (instead of just last token)\n",
    "    else:\n",
    "        token_classes = ['demonstration', 'label', 'separator', 'predictive', 'structural','end_of_example', \n",
    "                        'query_demonstration', 'query_structural', 'query_separator', 'query_predictive']\n",
    "        token_classes_regex = ['demonstration_[\\d]{1,}_token', 'demonstration_[\\d]{1,}_label_token', 'separator_token', 'predictive_token', 'structural_token','end_of_example_token', \n",
    "                            'query_demonstration_token', 'query_structural_token', 'query_separator_token', 'query_predictive_token']\n",
    "    \n",
    "\n",
    "    indirect_effect_storage = torch.zeros(model_config['n_layers'], model_config['n_heads'],len(token_classes))\n",
    "    print(f\"indirect_effect_storage.shape: {indirect_effect_storage.shape}\")\n",
    "\n",
    "    # Clean Run of Baseline:\n",
    "    clean_output = model(**inputs).logits[:,-1,:]\n",
    "    print(f\"clean_output.shape: {clean_output.shape}\")\n",
    "    clean_probs = torch.softmax(clean_output[0], dim=-1)\n",
    "    print(f\"clean_probs.shape: {clean_probs.shape}\")\n",
    "\n",
    "    # For every layer, head, token combination perform the replacement & track the change in meaningful tokens\n",
    "    for layer in range(model_config['n_layers']):\n",
    "        head_hook_layer = [model_config['attn_hook_names'][layer]]\n",
    "        \n",
    "        for head_n in range(model_config['n_heads']):\n",
    "            for i,(token_class, class_regex) in enumerate(zip(token_classes, token_classes_regex)):\n",
    "                print(\"token_class, class_regex: \", token_class, class_regex)\n",
    "                reg_class_match = re.compile(f\"^{class_regex}$\")\n",
    "                print(\"reg_class_match: \", reg_class_match)\n",
    "                \n",
    "                class_token_inds = [x[0] for x in token_labels if reg_class_match.match(x[2])]\n",
    "                print(\"class_token_inds: \", class_token_inds)\n",
    "                \n",
    "\n",
    "                intervention_locations = [(layer, head_n, token_n) for token_n in class_token_inds]\n",
    "                print(\"intervention_locations:\", intervention_locations)\n",
    "                intervention_fn = replace_activation_w_avg(layer_head_token_pairs=intervention_locations, avg_activations=avg_activations, \n",
    "                                                           model=model, model_config=model_config,\n",
    "                                                           batched_input=False, idx_map=idx_map, last_token_only=last_token_only)\n",
    "                with TraceDict(model, layers=head_hook_layer, edit_output=intervention_fn) as td:                \n",
    "                    output = model(**inputs).logits[:,-1,:] # batch_size x n_tokens x vocab_size, only want last token prediction\n",
    "                \n",
    "                # TRACK probs of tokens of interest\n",
    "                intervention_probs = torch.softmax(output, dim=-1) # convert to probability distribution\n",
    "                indirect_effect_storage[layer,head_n,i] = (intervention_probs-clean_probs).index_select(1, torch.LongTensor(token_id_of_interest).to(device).squeeze()).squeeze()\n",
    "\n",
    "    return indirect_effect_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def word_pairs_to_prompt_data(word_pairs : dict,\n",
    "                              instructions: str = \"\",\n",
    "                              prefixes: dict = {\"input\":\"Q:\", \"output\":\"A:\",\"instructions\":\"\"},\n",
    "                              separators: dict = {\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"},\n",
    "                              query_target_pair: dict = None, prepend_bos_token=False,\n",
    "                              shuffle_labels=False, prepend_space=True) -> dict:\n",
    "    \"\"\"Takes a dataset of word pairs, and constructs a prompt_data dict with additional information to construct an ICL prompt.\n",
    "    Parameters:\n",
    "    word_pairs: dict of the form {'word1':['a', 'b', ...], 'word2':['c', 'd', ...]}\n",
    "    instructions: prefix instructions for an ICL prompt\n",
    "    prefixes: dict of ICL prefixes that are prepended to inputs, outputs and instructions\n",
    "    separators: dict of ICL separators that are appended to inputs, outputs and instructions\n",
    "    query_target_pair: dict with a single input-output pair acting as the query for the prompt\n",
    "    prepend_bos_token: whether or not to prepend a BOS token to the prompt\n",
    "    shuffle_labels: whether to shuffle the ICL labels\n",
    "    prepend_space: whether to prepend a space to every input and output token\n",
    "\n",
    "    Returns: \n",
    "    prompt_data: dict containing ICL prompt examples, and template information\n",
    "    \"\"\"\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    prompt_data = {}\n",
    "    prompt_data['instructions'] = instructions\n",
    "    print(\"Instructions:\", instructions)\n",
    "    prompt_data['separators'] = separators\n",
    "    print(\"Separators:\", separators)\n",
    "    if prepend_bos_token:\n",
    "        prefixes = {k:(v if k !='instructions' else '' + v) for (k,v) in prefixes.items()}\n",
    "    prompt_data['prefixes'] = prefixes\n",
    "    print(\"Prefixes:\", prefixes)\n",
    "\n",
    "    if query_target_pair is not None:\n",
    "        query_target_pair = {k:(v[0] if isinstance(v, list) else v) for k,v in query_target_pair.items()}\n",
    "    prompt_data['query_target'] = query_target_pair\n",
    "    print(\"Query Target Pair:\", query_target_pair)\n",
    "        \n",
    "    if shuffle_labels:\n",
    "        randomized_pairs = [np.random.permutation(x).tolist() if i==1 else x for (i,x) in enumerate(list(word_pairs.values()))] # shuffle labels only\n",
    "        if prepend_space:\n",
    "            prompt_data['examples'] = [{'input':' ' + w1, 'output':' ' + w2} for (w1,w2) in list(zip(*randomized_pairs))]\n",
    "            prompt_data['query_target'] = {k:' ' + v for k,v in query_target_pair.items()} if query_target_pair is not None else None\n",
    "        else:\n",
    "            prompt_data['examples'] = [{'input':w1, 'output':w2} for (w1,w2) in list(zip(*randomized_pairs))]\n",
    "    else:    \n",
    "        if prepend_space:\n",
    "            prompt_data['examples'] = [{'input':' ' + w1, 'output':' ' + str(w2)} for (w1,w2) in list(zip(*word_pairs.values()))]\n",
    "            prompt_data['query_target'] = {k:' ' + str(v) for k,v in query_target_pair.items()} if query_target_pair is not None else None\n",
    "        else:\n",
    "            prompt_data['examples'] = [{'input':w1, 'output':w2} for (w1,w2) in list(zip(*word_pairs.values()))]\n",
    "    \n",
    "    print(\"Prompt Data Examples:\", prompt_data['examples'])\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    return prompt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_indirect_effect(dataset, mean_activations, model, model_config, tokenizer, n_shots=10, n_trials=25, last_token_only=True, prefixes=None, separators=None, filter_set=None):\n",
    "    \"\"\"\n",
    "    Computes Indirect Effect of each head in the model\n",
    "    \"\"\"\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    print(f\"Starting computation of indirect effects with {n_shots} shots and {n_trials} trials...\")\n",
    "    n_test_examples = 1\n",
    "\n",
    "    if prefixes is not None and separators is not None:\n",
    "        print(\"Using custom prefixes and separators for dummy token labels...\")\n",
    "        dummy_gt_labels = get_dummy_token_labels(n_shots, tokenizer=tokenizer, prefixes=prefixes, separators=separators)\n",
    "    else:\n",
    "        print(\"Using default settings for dummy token labels...\")\n",
    "        dummy_gt_labels = get_dummy_token_labels(n_shots, tokenizer=tokenizer)\n",
    "    \n",
    "    print(f\"Dummy ground truth labels generated with length: {len(dummy_gt_labels)}\")\n",
    "    print(f\" 10 Dummy ground truth labels are:{dummy_gt_labels[:10]}\")\n",
    "    \n",
    "    is_llama = 'llama' in model_config['name_or_path']\n",
    "    prepend_bos = not is_llama\n",
    "    print(f\"Model prepend_bos setting: {prepend_bos}\")\n",
    "\n",
    "    if last_token_only:\n",
    "        indirect_effect = torch.zeros(n_trials, model_config['n_layers'], model_config['n_heads'])\n",
    "    else:\n",
    "        indirect_effect = torch.zeros(n_trials, model_config['n_layers'], model_config['n_heads'], 10) # have 10 classes of tokens\n",
    "    \n",
    "    print(f\"Initialized indirect effect tensor with zero tensor of shape: {indirect_effect.shape}\")\n",
    "\n",
    "    if filter_set is None:\n",
    "        filter_set = np.arange(len(dataset['valid']))\n",
    "    \n",
    "    for i in tqdm(range(n_trials), total=n_trials):\n",
    "        word_pairs = dataset['train'][np.random.choice(len(dataset['train']), n_shots, replace=False)]\n",
    "        word_pairs_test = dataset['valid'][np.random.choice(filter_set, n_test_examples, replace=False)]\n",
    "        \n",
    "        print(f\"Trial {i+1}/{n_trials}: Selected word pairs for prompts and testing.\")\n",
    "\n",
    "        print(\"Parameters to word_pairs_to_prompt_data:\")\n",
    "        print(f\"word_pairs: {word_pairs}, query_target_pair: {word_pairs_test}, shuffle_labels: True, prepend_bos_token: {prepend_bos}\")\n",
    "\n",
    "        if prefixes is not None and separators is not None:\n",
    "            prompt_data_random = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, shuffle_labels=True, \n",
    "                                                           prepend_bos_token=prepend_bos, prefixes=prefixes, separators=separators)\n",
    "        else:\n",
    "            prompt_data_random = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, \n",
    "                                                           shuffle_labels=True, prepend_bos_token=prepend_bos)\n",
    "        print(f\"Generated prompt data for trial {i+1}.\")\n",
    "        print(\"Prompt Data\", prompt_data_random)\n",
    "        \n",
    "        ind_effects = activation_replacement_per_class_intervention(prompt_data=prompt_data_random, \n",
    "                                                                    avg_activations=mean_activations, \n",
    "                                                                    dummy_labels=dummy_gt_labels, \n",
    "                                                                    model=model, model_config=model_config, tokenizer=tokenizer, \n",
    "                                                                    last_token_only=last_token_only)\n",
    "        indirect_effect[i] = ind_effects.squeeze()\n",
    "        print(f\"Indirect effects calculated for trial {i+1} with shape: {ind_effects.shape}.\")\n",
    "\n",
    "    print(\"Completed computation of indirect effects.\")\n",
    "    return indirect_effect\n",
    "\n",
    "# Note: You'll need to define or adapt the functions get_dummy_token_labels and word_pairs_to_prompt_data,\n",
    "# as well as activation_replacement_per_class_intervention, according to their requirements and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = {\n",
    "    'dataset_name': 'antonym',\n",
    "    'model_name': 'gpt2',\n",
    "    'root_data_dir': '../dataset_files',\n",
    "    'save_path_root': '../results',\n",
    "    'seed': 42,\n",
    "    'n_shots': 10,\n",
    "    'n_trials': 1,\n",
    "    'test_split': 0.3,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'mean_activations_path': None,\n",
    "    'last_token_only': True,\n",
    "    'prefixes': {\"input\": \"Q:\", \"output\": \"A:\", \"instructions\": \"\"},\n",
    "    'separators': {\"input\": \"\\\\n\", \"output\": \"\\\\n\\\\n\", \"instructions\": \"\"}\n",
    "}\n",
    "\n",
    "dataset_name = args['dataset_name']\n",
    "model_name = args['model_name']\n",
    "root_data_dir = args['root_data_dir']\n",
    "save_path_root = f\"{args['save_path_root']}/{dataset_name}\"\n",
    "seed = args['seed']\n",
    "n_shots = args['n_shots']\n",
    "n_trials = args['n_trials']\n",
    "test_split = args['test_split']\n",
    "device = args['device']\n",
    "mean_activations_path = args['mean_activations_path']\n",
    "last_token_only = args['last_token_only']\n",
    "prefixes = args['prefixes']\n",
    "separators = args['separators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Loading:  gpt2\n"
     ]
    }
   ],
   "source": [
    "# Load Model & Tokenizer\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"Loading Model\")\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading Dataset\")\n",
    "dataset = load_dataset(dataset_name, root_data_dir=root_data_dir, test_size=test_split, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path_root):\n",
    "    os.makedirs(save_path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or Re-Compute Mean Activations\n",
    "if mean_activations_path is not None and os.path.exists(mean_activations_path):\n",
    "    mean_activations = torch.load(mean_activations_path)\n",
    "elif mean_activations_path is None and os.path.exists(f'{save_path_root}/{dataset_name}_mean_head_activations.pt'):\n",
    "    mean_activations_path = f'{save_path_root}/{dataset_name}_mean_head_activations.pt'\n",
    "    mean_activations = torch.load(mean_activations_path)        \n",
    "else:\n",
    "    print(\"Computing Mean Activations\")\n",
    "    mean_activations = get_mean_head_activations(dataset, model=model, model_config=model_config, tokenizer=tokenizer, \n",
    "                                                    n_icl_examples=n_shots, N_TRIALS=n_trials, prefixes=prefixes, separators=separators)\n",
    "    torch.save(mean_activations, f'{save_path_root}/{dataset_name}_mean_head_activations.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Q:', 'output': 'A:', 'instructions': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '\\\\n', 'output': '\\\\n\\\\n', 'instructions': ''}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Indirect Effect\n",
      "---------------------------------------------------------------------\n",
      "Starting computation of indirect effects with 10 shots and 1 trials...\n",
      "Using custom prefixes and separators for dummy token labels...\n",
      "Dummy ground truth labels generated with length: 128\n",
      " 10 Dummy ground truth labels are:[(0, 'bos_token'), (1, 'structural_token'), (2, 'structural_token'), (3, 'demonstration_1_token'), (4, 'separator_token'), (5, 'separator_token'), (6, 'structural_token'), (7, 'predictive_token'), (8, 'demonstration_1_label_token'), (9, 'end_of_example_token')]\n",
      "Model prepend_bos setting: True\n",
      "Initialized indirect effect tensor with zero tensor of shape: torch.Size([1, 12, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/1: Selected word pairs for prompts and testing.\n",
      "Parameters to word_pairs_to_prompt_data:\n",
      "word_pairs: {'input': ['valley', 'economic', 'fugitive', 'inward', 'unmarked', 'worthless', 'regress', 'borrowing', 'saline', 'risky'], 'output': ['mountain', 'uneconomic', 'law-abiding citizen', 'outward', 'marked', 'valuable', 'progress', 'lending', 'freshwater', 'safe']}, query_target_pair: {'input': ['secure'], 'output': ['insecure']}, shuffle_labels: True, prepend_bos_token: True\n",
      "---------------------------------------------------------------------\n",
      "Instructions: \n",
      "Separators: {'input': '\\\\n', 'output': '\\\\n\\\\n', 'instructions': ''}\n",
      "Prefixes: {'input': 'Q:', 'output': 'A:', 'instructions': ''}\n",
      "Query Target Pair: {'input': 'secure', 'output': 'insecure'}\n",
      "Prompt Data Examples: [{'input': ' valley', 'output': ' progress'}, {'input': ' economic', 'output': ' valuable'}, {'input': ' fugitive', 'output': ' outward'}, {'input': ' inward', 'output': ' freshwater'}, {'input': ' unmarked', 'output': ' safe'}, {'input': ' worthless', 'output': ' mountain'}, {'input': ' regress', 'output': ' marked'}, {'input': ' borrowing', 'output': ' lending'}, {'input': ' saline', 'output': ' uneconomic'}, {'input': ' risky', 'output': ' law-abiding citizen'}]\n",
      "---------------------------------------------------------------------\n",
      "Generated prompt data for trial 1.\n",
      "Prompt Data {'instructions': '', 'separators': {'input': '\\\\n', 'output': '\\\\n\\\\n', 'instructions': ''}, 'prefixes': {'input': 'Q:', 'output': 'A:', 'instructions': ''}, 'query_target': {'input': ' secure', 'output': ' insecure'}, 'examples': [{'input': ' valley', 'output': ' progress'}, {'input': ' economic', 'output': ' valuable'}, {'input': ' fugitive', 'output': ' outward'}, {'input': ' inward', 'output': ' freshwater'}, {'input': ' unmarked', 'output': ' safe'}, {'input': ' worthless', 'output': ' mountain'}, {'input': ' regress', 'output': ' marked'}, {'input': ' borrowing', 'output': ' lending'}, {'input': ' saline', 'output': ' uneconomic'}, {'input': ' risky', 'output': ' law-abiding citizen'}]}\n",
      "---------------------------Activation Replacement--------------\n",
      "prompt_data: {'instructions': '', 'separators': {'input': '\\\\n', 'output': '\\\\n\\\\n', 'instructions': ''}, 'prefixes': {'input': 'Q:', 'output': 'A:', 'instructions': ''}, 'query_target': {'input': ' secure', 'output': ' insecure'}, 'examples': [{'input': ' valley', 'output': ' progress'}, {'input': ' economic', 'output': ' valuable'}, {'input': ' fugitive', 'output': ' outward'}, {'input': ' inward', 'output': ' freshwater'}, {'input': ' unmarked', 'output': ' safe'}, {'input': ' worthless', 'output': ' mountain'}, {'input': ' regress', 'output': ' marked'}, {'input': ' borrowing', 'output': ' lending'}, {'input': ' saline', 'output': ' uneconomic'}, {'input': ' risky', 'output': ' law-abiding citizen'}]}\n",
      "token_labels: [(0, 'Q', 'structural_token'), (1, ':', 'structural_token'), (2, ' valley', 'demonstration_1_token'), (3, '\\\\', 'separator_token'), (4, 'n', 'separator_token'), (5, 'A', 'structural_token'), (6, ':', 'predictive_token'), (7, ' progress', 'demonstration_1_label_token'), (8, '\\\\', 'end_of_example_token'), (9, 'n', 'separator_token'), (10, '\\\\', 'separator_token'), (11, 'n', 'separator_token'), (12, 'Q', 'structural_token'), (13, ':', 'structural_token'), (14, ' economic', 'demonstration_2_token'), (15, '\\\\', 'separator_token'), (16, 'n', 'separator_token'), (17, 'A', 'structural_token'), (18, ':', 'predictive_token'), (19, ' valuable', 'demonstration_2_label_token'), (20, '\\\\', 'end_of_example_token'), (21, 'n', 'separator_token'), (22, '\\\\', 'separator_token'), (23, 'n', 'separator_token'), (24, 'Q', 'structural_token'), (25, ':', 'structural_token'), (26, ' fugitive', 'demonstration_3_token'), (27, '\\\\', 'separator_token'), (28, 'n', 'separator_token'), (29, 'A', 'structural_token'), (30, ':', 'predictive_token'), (31, ' outward', 'demonstration_3_label_token'), (32, '\\\\', 'end_of_example_token'), (33, 'n', 'separator_token'), (34, '\\\\', 'separator_token'), (35, 'n', 'separator_token'), (36, 'Q', 'structural_token'), (37, ':', 'structural_token'), (38, ' inward', 'demonstration_4_token'), (39, '\\\\', 'separator_token'), (40, 'n', 'separator_token'), (41, 'A', 'structural_token'), (42, ':', 'predictive_token'), (43, ' freshwater', 'demonstration_4_label_token'), (44, '\\\\', 'end_of_example_token'), (45, 'n', 'separator_token'), (46, '\\\\', 'separator_token'), (47, 'n', 'separator_token'), (48, 'Q', 'structural_token'), (49, ':', 'structural_token'), (50, ' unmarked', 'demonstration_5_token'), (51, '\\\\', 'separator_token'), (52, 'n', 'separator_token'), (53, 'A', 'structural_token'), (54, ':', 'predictive_token'), (55, ' safe', 'demonstration_5_label_token'), (56, '\\\\', 'end_of_example_token'), (57, 'n', 'separator_token'), (58, '\\\\', 'separator_token'), (59, 'n', 'separator_token'), (60, 'Q', 'structural_token'), (61, ':', 'structural_token'), (62, ' worthless', 'demonstration_6_token'), (63, '\\\\', 'separator_token'), (64, 'n', 'separator_token'), (65, 'A', 'structural_token'), (66, ':', 'predictive_token'), (67, ' mountain', 'demonstration_6_label_token'), (68, '\\\\', 'end_of_example_token'), (69, 'n', 'separator_token'), (70, '\\\\', 'separator_token'), (71, 'n', 'separator_token'), (72, 'Q', 'structural_token'), (73, ':', 'structural_token'), (74, ' regress', 'demonstration_7_token'), (75, '\\\\', 'separator_token'), (76, 'n', 'separator_token'), (77, 'A', 'structural_token'), (78, ':', 'predictive_token'), (79, ' marked', 'demonstration_7_label_token'), (80, '\\\\', 'end_of_example_token'), (81, 'n', 'separator_token'), (82, '\\\\', 'separator_token'), (83, 'n', 'separator_token'), (84, 'Q', 'structural_token'), (85, ':', 'structural_token'), (86, ' borrowing', 'demonstration_8_token'), (87, '\\\\', 'separator_token'), (88, 'n', 'separator_token'), (89, 'A', 'structural_token'), (90, ':', 'predictive_token'), (91, ' lending', 'demonstration_8_label_token'), (92, '\\\\', 'end_of_example_token'), (93, 'n', 'separator_token'), (94, '\\\\', 'separator_token'), (95, 'n', 'separator_token'), (96, 'Q', 'structural_token'), (97, ':', 'structural_token'), (98, ' saline', 'demonstration_9_token'), (99, '\\\\', 'separator_token'), (100, 'n', 'separator_token'), (101, 'A', 'structural_token'), (102, ':', 'predictive_token'), (103, ' un', 'demonstration_9_label_token'), (104, 'economic', 'demonstration_9_label_token'), (105, '\\\\', 'end_of_example_token'), (106, 'n', 'separator_token'), (107, '\\\\', 'separator_token'), (108, 'n', 'separator_token'), (109, 'Q', 'structural_token'), (110, ':', 'structural_token'), (111, ' risky', 'demonstration_10_token'), (112, '\\\\', 'separator_token'), (113, 'n', 'separator_token'), (114, 'A', 'structural_token'), (115, ':', 'predictive_token'), (116, ' law', 'demonstration_10_label_token'), (117, '-', 'demonstration_10_label_token'), (118, 'abiding', 'demonstration_10_label_token'), (119, ' citizen', 'demonstration_10_label_token'), (120, '\\\\', 'end_of_example_token'), (121, 'n', 'separator_token'), (122, '\\\\', 'separator_token'), (123, 'n', 'separator_token'), (124, 'Q', 'query_structural_token'), (125, ':', 'query_structural_token'), (126, ' secure', 'query_demonstration_token'), (127, '\\\\', 'query_separator_token'), (128, 'n', 'query_separator_token'), (129, 'A', 'query_structural_token'), (130, ':', 'query_predictive_token')]\n",
      "prompt_string: Q: valley\\nA: progress\\n\\nQ: economic\\nA: valuable\\n\\nQ: fugitive\\nA: outward\\n\\nQ: inward\\nA: freshwater\\n\\nQ: unmarked\\nA: safe\\n\\nQ: worthless\\nA: mountain\\n\\nQ: regress\\nA: marked\\n\\nQ: borrowing\\nA: lending\\n\\nQ: saline\\nA: uneconomic\\n\\nQ: risky\\nA: law-abiding citizen\\n\\nQ: secure\\nA:\n",
      "indirect_effect_storage.shape: torch.Size([12, 12, 1])\n",
      "clean_output.shape: torch.Size([1, 50257])\n",
      "clean_probs.shape: torch.Size([50257])\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(0, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(1, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(2, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(3, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(4, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(5, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(6, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(7, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(8, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(9, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 10, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(10, 11, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 0, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 1, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 2, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 3, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 4, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 5, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 6, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 7, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 8, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 9, 130)]\n",
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 10, 130)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:02<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_class, class_regex:  query_predictive query_predictive_token\n",
      "reg_class_match:  re.compile('^query_predictive_token$')\n",
      "class_token_inds:  [130]\n",
      "intervention_locations: [(11, 11, 130)]\n",
      "Indirect effects calculated for trial 1 with shape: torch.Size([12, 12, 1]).\n",
      "Completed computation of indirect effects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing Indirect Effect\")\n",
    "indirect_effect = compute_indirect_effect(dataset, mean_activations, model=model, model_config=model_config, tokenizer=tokenizer, \n",
    "                                            n_shots=n_shots, n_trials=n_trials, last_token_only=last_token_only, prefixes=prefixes, separators=separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write args to file\n",
    "args['save_path_root'] = save_path_root\n",
    "args['mean_activations_path'] = mean_activations_path\n",
    "with open(f'{save_path_root}/indirect_effect_args.txt', 'w') as arg_file:\n",
    "    json.dump(args, arg_file, indent=2)\n",
    "\n",
    "torch.save(indirect_effect, f'{save_path_root}/{dataset_name}_indirect_effect.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
